{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from scipy.ndimage import center_of_mass\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory and mapping file\n",
    "data_dir = '/home/zaz22/research-data-store/fmri/fmri_beijing'\n",
    "mapping_file = '/home/zaz22/research-data-store/rois/rois_3000_beijing/rois/brain_atoms.mnc.gz'  \n",
    "\n",
    "# Define repetition time (TR)\n",
    "repetition_time = 2.0\n",
    "\n",
    "# List of subject IDs\n",
    "# subject_ids = ['9640133', '9783279', '9887336', '9890726', '4095748', '4136226', '4221029',\n",
    "#                 '4225073', '4241194', '4256491', '4265987', '4334113', '4383707', '4475709',\n",
    "#                 '4921428', '5150328', '5193577', '5575344', '5600820', '5669389', '5993008',\n",
    "#                 '6187322', '6383713', '6477085', '6500128', '7011503', '7093319', '7135128',\n",
    "#                 '7253183', '7390867', '7407032', '7689953', '7994085', '8191384', '8278680',]\n",
    "\n",
    "# subject_ids = [\n",
    "#     '9640133', '9783279', '9887336', '9890726', '4095748', '4136226', '4221029',\n",
    "#     '4225073', '4241194', '4256491', '4265987', '4334113', '4383707', '4475709',\n",
    "#     '4921428', '5150328', '5193577', '5575344', '5600820', '5669389', '5993008',\n",
    "#     '6187322', '6383713', '6477085', '6500128', '7011503', '7093319', '7135128',\n",
    "#     '7253183', '7390867', '7407032', '7689953', '7994085', '8191384', '8278680',\n",
    "#     '8328877', '8838009', '9002207', '9093997', '9210521', '9221927', '9578631',\n",
    "#     '3494778', '3554582', '3561920', '3562883', '3587000', '3593327', '3610134',\n",
    "#     '3624598', '3655623', '3672300', '3672854', '3691107', '3707771', '3712305',\n",
    "#     '3732101', '3739175', '3803759', '3809753', '3827352', '3856956', '3870624',\n",
    "#     '3889095', '3910672', '3930512', '3967265', '3976121', '3983607', '3993793',\n",
    "#     '3994098', '4006710', '4028266', '4048810', '4053388', '4053836', '4055710',\n",
    "#     '4073815', '4075719', '4091983', '2538839', '2559537', '2601519', '2659769',\n",
    "#     '2697768', '2703336', '2714224', '2737106', '2780647', '2833684', '2884672',\n",
    "#     '2897046', '2907951', '2910270', '2919220', '2940712', '2950754', '2984158',\n",
    "#     '3004580', '3086074', '3157406', '3194757', '3205761', '3212536', '3224401',\n",
    "#     '3233028', '3239413', '3248920', '3262042', '3269608', '3277313', '3291029',\n",
    "#     '3306863', '3308331', '3385520', '3390312', '3446674', '3473830', '1809715',\n",
    "#     '1843546', '1860323', '1875013', '1875711', '1879542', '1912810', '1916266',\n",
    "#     '1947991', '2031422', '2033178', '2081754', '2106109', '2107404', '2123983',\n",
    "#     '2140063', '2141250', '2174595', '2196753', '2207418', '2208591', '2228148',\n",
    "#     '2240562', '2249443', '2266806', '2268253', '2276801', '2296326', '2310449',\n",
    "#     '2367157', '2377207', '2408774', '2427408', '2493190', '2498847', '2524687',\n",
    "#     '2529026', '2535087', '1050345', '1050975', '1056121', '1068505', '1093743',\n",
    "#     '1094669', '1113498', '1117299', '1132854', '1133221', '1139030', '1159908',\n",
    "#     '1177160', '1186237', '1240299', '1258069', '1282248', '1302449', '1341865',\n",
    "#     '1356553', '1391181', '1399863', '1404738', '1408093', '1411536', '1469171',\n",
    "#     '1494102', '1561488', '1562298', '1628610', '1643780', '1662160', '1686092',\n",
    "#     '1689948', '1771270', '1791543', '1794770', '1805037'\n",
    "# ]\n",
    "\n",
    "subject_ids = [\n",
    "    '9640133', '9783279', '9887336', '9890726', '4095748', '4136226', '4221029',\n",
    "    '4225073', '4241194', '4256491', '4265987', '4334113', '4383707', '4475709',\n",
    "    '4921428', '5150328', '5193577', '5575344', '5600820', '5669389', '5993008',\n",
    "    '6187322', '6383713', '6477085', '6500128', '7011503', '7093319', '7135128',\n",
    "    '7253183', '7390867', '7407032', '7689953', '7994085', '8191384', '8278680',\n",
    "    '8328877', '8838009', '9002207', '9093997', '9210521', '9221927', '9578631',\n",
    "    '3494778', '3554582', '3561920', '3562883', '3587000', '3593327', '3610134',\n",
    "    '3624598', '3655623', '3672300', '3672854', '3691107', '3707771', '3712305',\n",
    "    '3732101', '3739175', '3803759', '3809753', '3827352', '3856956', '3870624',\n",
    "    '3889095', '3910672', '3930512', '3967265', '3976121', '3983607', '3993793',\n",
    "    '3994098', '4006710', '4028266', '4048810', '4053388', '4053836', '4055710',\n",
    "    '4073815', '4075719', '4091983', '2538839', '2559537', '2601519', '2659769',\n",
    "    '2697768', '2703336', '2714224', '2737106', '2780647', '2833684', '2884672',\n",
    "    '2897046', '2907951', '2910270', '2919220', '2940712', '2950754', '2984158',\n",
    "    '3004580', '3086074', '3157406', '3194757', '3205761', '3212536', '3224401'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ROI mapping file\n",
    "mapping_img = nib.load(mapping_file)\n",
    "mapping_data = mapping_img.get_fdata()\n",
    "affine = mapping_img.affine\n",
    "\n",
    "# Get unique ROI labels, excluding 0\n",
    "roi_labels = np.unique(mapping_data)\n",
    "roi_labels = roi_labels[roi_labels != 0]\n",
    "print(f'Number of ROIs: {len(roi_labels)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ROI coordinates (center of mass)\n",
    "roi_coords = []\n",
    "\n",
    "for label in roi_labels:\n",
    "    # Create a binary mask for the current ROI\n",
    "    roi_mask = mapping_data == label\n",
    "\n",
    "    # Calculate the center of mass in voxel space\n",
    "    com_voxel = center_of_mass(roi_mask)\n",
    "\n",
    "    # Convert voxel coordinates to world coordinates\n",
    "    com_world = nib.affines.apply_affine(affine, com_voxel)\n",
    "\n",
    "    roi_coords.append(com_world)\n",
    "\n",
    "print(f'ROI coordinates (world space): {roi_coords}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a masker for extracting time series from ROIs\n",
    "labels_masker = NiftiLabelsMasker(\n",
    "    labels_img=mapping_img,\n",
    "    standardize=True,\n",
    "    detrend=True,\n",
    "    t_r=repetition_time,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Initialize a list to store time series for all subjects\n",
    "all_tseries = []\n",
    "\n",
    "for sub_id in subject_ids:\n",
    "    # Build the functional file path\n",
    "    func_file = os.path.join(data_dir, f'fmri_X_{sub_id}_session_1_run1.nii.gz')\n",
    "    print(f'Loading functional image for subject {sub_id} from {func_file}')\n",
    "    \n",
    "    # Load the functional image\n",
    "    func_img = nib.load(func_file)\n",
    "    \n",
    "    # Extract time series using the labels masker\n",
    "    tseries = labels_masker.fit_transform(func_img)\n",
    "    \n",
    "    all_tseries.append(tseries)\n",
    "    print(f'Extracted time series for subject {sub_id}, shape: {tseries.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths and parameters\n",
    "behavioral_file = '/home/zaz22/repos/adhd-mri-using-ml/adhd200_preprocessed_phenotypics.tsv'  # Update with your actual file path\n",
    "behavioral = pd.read_csv(behavioral_file, sep='\\t')\n",
    "\n",
    "# Define diagnosis groups\n",
    "behavioral['DX'] = behavioral['DX'].astype(str).str.strip()\n",
    "\n",
    "# Apply mapping\n",
    "conditions_map = {\n",
    "    '0': 'Non-ADHD',\n",
    "    '1': 'ADHD',\n",
    "    '2': 'ADHD',\n",
    "    '3': 'ADHD'\n",
    "}\n",
    "behavioral['Diagnosis_Class'] = behavioral['DX'].map(conditions_map)\n",
    "print(behavioral['Diagnosis_Class'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the diagnosis column to binary classes\n",
    "behavioral['Diagnosis_Class'] = behavioral['DX'].map(conditions_map)\n",
    "behavioral = behavioral.loc[behavioral['DX'] != 'pending']\n",
    "print(behavioral['DX'].unique())  # Check all unique values in the DX column\n",
    "print(behavioral['Diagnosis_Class'].value_counts())  # Check how the mapping works\n",
    "print(behavioral[behavioral['DX'].isna()])  # Check rows where DX is NaN\n",
    "print(behavioral['Diagnosis_Class'].unique())  # Check unique mapped values\n",
    "print(behavioral['DX'].dtypes)  # Check if the column is numeric or string\n",
    "print(behavioral['DX'].unique())  # Confirm all unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for valid diagnosis classes\n",
    "condition_mask = behavioral['Diagnosis_Class'].notna()\n",
    "print(f\"Condition mask shape: {condition_mask.shape}\")\n",
    "print(condition_mask.sum())  # Check how many rows are being selected\n",
    "\n",
    "filtered_behavioral_data = behavioral[condition_mask]\n",
    "print(f\"Behavioral data shape after filtering: {filtered_behavioral_data.shape}\")\n",
    "\n",
    "# Extract valid subject IDs that match 'subject_ids'\n",
    "valid_subject_ids = [\n",
    "    sub_id for sub_id in subject_ids if sub_id in filtered_behavioral_data['ScanDir ID'].astype(str).tolist()\n",
    "]\n",
    "print(f\"Valid Subject IDs: {valid_subject_ids}\")\n",
    "\n",
    "# Filter fMRI images using the valid subject IDs\n",
    "valid_fmri_imgs = []\n",
    "\n",
    "for subject_id in valid_subject_ids:\n",
    "    # Build the path to the subject's fMRI file\n",
    "    fmri_path = os.path.join(data_dir, f'fmri_X_{subject_id}_session_1_run1.nii.gz')\n",
    "\n",
    "    # Check if the file exists before attempting to load\n",
    "    if not os.path.exists(fmri_path):\n",
    "        print(f\"File not found for Subject ID: {subject_id}\")\n",
    "        continue\n",
    "\n",
    "    # Load the subject's fMRI image\n",
    "    fmri_img = nib.load(fmri_path)\n",
    "    valid_fmri_imgs.append(fmri_img)\n",
    "\n",
    "# Function to check and correct shapes\n",
    "from nilearn.image import resample_img, concat_imgs\n",
    "\n",
    "def check_and_correct_shapes(image_list):\n",
    "    if not image_list:\n",
    "        raise ValueError(\"The image list is empty.\")\n",
    "    \n",
    "    # Reference shape and affine from the first image\n",
    "    reference_img = image_list[0]\n",
    "    target_shape = reference_img.shape[:3]  # Only spatial dimensions\n",
    "    target_affine = reference_img.affine\n",
    "\n",
    "    corrected_images = []\n",
    "    for idx, img in enumerate(image_list):\n",
    "        if img.shape[:3] != target_shape:\n",
    "            print(f\"Resampling image {idx} to match target shape {target_shape}\")\n",
    "            img = resample_img(img, target_affine=target_affine, target_shape=target_shape)\n",
    "        corrected_images.append(img)\n",
    "    \n",
    "    return corrected_images\n",
    "\n",
    "# Check and correct fMRI image shapes\n",
    "try:\n",
    "    valid_fmri_imgs = check_and_correct_shapes(valid_fmri_imgs)\n",
    "    # Concatenate all valid fMRI images into a single 4D image\n",
    "    fmri_imgs = concat_imgs(valid_fmri_imgs)\n",
    "    print(f\"Concatenated fMRI images shape: {fmri_imgs.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during processing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import index_img\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nilearn.decoding import Decoder\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Ensure fmri_imgs and valid_subject_ids are already defined\n",
    "print(f\"fMRI data shape: {fmri_imgs.shape}\")\n",
    "\n",
    "# Extract the relevant conditions\n",
    "conditions = filtered_behavioral_data['Diagnosis_Class'].values\n",
    "print(f\"Conditions shape: {conditions.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the decoder with desired parameters\n",
    "\n",
    "# REGRESSION: \n",
    "# For regression, valid entries are: ‘r2’, ‘neg_mean_absolute_error’, or ‘neg_mean_squared_error’.\n",
    "\n",
    "        # DEFAULT DECODER:\n",
    "        # Use when you have a small-to-medium dataset with potentially non-linear decision boundaries.\n",
    "        # Great for clean data with clearly separable classes.\n",
    "#decoder = Decoder(estimator=\"svc\", standardize=\"zscore_sample\", cv=5, scoring=\"accuracy\",n_jobs=16)\n",
    "\n",
    "        # L2 DECODER:\n",
    "        # l2 regularization penalizes the sum of the squares of the weights.\n",
    "        # L2 regularization keeps all features but reduces their influence\n",
    "# decoder = Decoder(estimator=\"logistic_l2\", standardize=\"zscore_sample\", cv=5, scoring=\"accuracy\",n_jobs=16)\n",
    "\n",
    "        # L1 DECODER: \n",
    "        # L1 regularization reduces model complexity by eliminating some features, \n",
    "        # L1 regularization penalizes the sum of the absolute values of the weights\n",
    "# logistic_l1 = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=10000)\n",
    "decoder = Decoder(estimator=\"logistic_l1\", standardize=\"zscore_sample\", cv=10, scoring=\"accuracy\",n_jobs=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"decoder.cv_scores_ keys:\", decoder.cv_scores_.keys())\n",
    "# print(\"decoder.cv_scores_ content:\", decoder.cv_scores_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(conditions), start=1):\n",
    "    # Split the data into training and test sets\n",
    "    fmri_train = index_img(fmri_imgs, train_idx)\n",
    "    fmri_test = index_img(fmri_imgs, test_idx)\n",
    "    conditions_train = conditions[train_idx]\n",
    "    conditions_test = conditions[test_idx]\n",
    "\n",
    "    # Train the decoder\n",
    "    decoder.fit(fmri_train, conditions_train)\n",
    "\n",
    "    # Calculate training loss\n",
    "    # Average scores across all labels and folds\n",
    "    mean_train_score = np.mean([\n",
    "        np.mean(scores) for scores in decoder.cv_scores_.values()\n",
    "    ])\n",
    "    train_loss = 1 - mean_train_score\n",
    "    training_losses.append(train_loss)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    predictions = decoder.predict(fmri_test)\n",
    "    \n",
    "    # Calculate validation loss (1 - test accuracy)\n",
    "    val_loss = 1 - accuracy_score(conditions_test, predictions)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "    # Calculate and store fold accuracy\n",
    "    accuracy = accuracy_score(conditions_test, predictions)\n",
    "    fold_accuracies.append(accuracy)\n",
    "    print(f\"Fold {fold}: Accuracy = {accuracy:.2f}, Training Loss = {train_loss:.2f}, Validation Loss = {val_loss:.2f}\")\n",
    "\n",
    "# Report overall accuracy\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"Mean Cross-Validation Accuracy: {mean_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting cross-validation accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(training_losses) + 1), training_losses, label='Training Loss', marker='o')\n",
    "plt.plot(range(1, len(validation_losses) + 1), validation_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs. Validation Loss Across Folds')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(fold_accuracies) + 1), fold_accuracies, label='Fold Accuracy', marker='o', color='green')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy Across Folds')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adhd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
