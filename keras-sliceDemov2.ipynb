{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Generate slices of fmri scans, then use a classfier to train on newly created slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLICE GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU details:  {'device_name': 'METAL'}\n"
     ]
    }
   ],
   "source": [
    "devices = tf.config.list_physical_devices()\n",
    "# print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  details = tf.config.experimental.get_device_details(gpus[0])\n",
    "  print(\"GPU details: \", details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = os.path.join('data','fmri','beijing')\n",
    "savepath = os.path.join('outputs','slice-outputs')\n",
    "fmri_files = Path(\"data/fmri/beijing\").glob(\"*.nii.gz\")\n",
    "count = 0\n",
    "\n",
    "def slice_gen(imagename,x):\n",
    "    \n",
    "    slice_data = test_img_data[:,:,20,x]\n",
    "\n",
    "    plt.imshow(slice_data, cmap='gray')\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.savefig(savepath + \"/\"+ imagename, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "for files in fmri_files:\n",
    "    imgname = 'output' + str(count) + '.png'\n",
    "    test_img = nib.load(str(files))\n",
    "    test_img_data = test_img.get_fdata()\n",
    "    #for x in range (0, 3):\n",
    "    slice_gen(imgname,count)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output0.png\n",
      "output1.png\n",
      "output10.png\n",
      "output100.png\n",
      "output101.png\n",
      "output102.png\n",
      "output103.png\n",
      "output104.png\n",
      "output105.png\n",
      "output106.png\n",
      "output107.png\n",
      "output108.png\n",
      "output109.png\n",
      "output11.png\n",
      "output110.png\n",
      "output111.png\n",
      "output112.png\n",
      "output113.png\n",
      "output114.png\n",
      "output115.png\n",
      "output116.png\n",
      "output117.png\n",
      "output118.png\n",
      "output119.png\n",
      "output12.png\n",
      "output120.png\n",
      "output121.png\n",
      "output122.png\n",
      "output123.png\n",
      "output124.png\n",
      "output125.png\n",
      "output126.png\n",
      "output127.png\n",
      "output128.png\n",
      "output129.png\n",
      "output13.png\n",
      "output130.png\n",
      "output131.png\n",
      "output132.png\n",
      "output133.png\n",
      "output134.png\n",
      "output135.png\n",
      "output136.png\n",
      "output137.png\n",
      "output138.png\n",
      "output139.png\n",
      "output14.png\n",
      "output140.png\n",
      "output141.png\n",
      "output142.png\n",
      "output143.png\n",
      "output144.png\n",
      "output145.png\n",
      "output146.png\n",
      "output147.png\n",
      "output148.png\n",
      "output149.png\n",
      "output15.png\n",
      "output150.png\n",
      "output151.png\n",
      "output152.png\n",
      "output153.png\n",
      "output154.png\n",
      "output155.png\n",
      "output156.png\n",
      "output157.png\n",
      "output158.png\n",
      "output159.png\n",
      "output16.png\n",
      "output160.png\n",
      "output161.png\n",
      "output162.png\n",
      "output163.png\n",
      "output164.png\n",
      "output165.png\n",
      "output166.png\n",
      "output167.png\n",
      "output168.png\n",
      "output169.png\n",
      "output17.png\n",
      "output170.png\n",
      "output171.png\n",
      "output172.png\n",
      "output173.png\n",
      "output174.png\n",
      "output175.png\n",
      "output176.png\n",
      "output177.png\n",
      "output178.png\n",
      "output179.png\n",
      "output18.png\n",
      "output180.png\n",
      "output181.png\n",
      "output182.png\n",
      "output183.png\n",
      "output184.png\n",
      "output185.png\n",
      "output186.png\n",
      "output187.png\n",
      "output188.png\n",
      "output189.png\n",
      "output19.png\n",
      "output190.png\n",
      "output191.png\n",
      "output192.png\n",
      "output193.png\n",
      "output2.png\n",
      "output20.png\n",
      "output21.png\n",
      "output22.png\n",
      "output23.png\n",
      "output24.png\n",
      "output25.png\n",
      "output26.png\n",
      "output27.png\n",
      "output28.png\n",
      "output29.png\n",
      "output3.png\n",
      "output30.png\n",
      "output31.png\n",
      "output32.png\n",
      "output33.png\n",
      "output34.png\n",
      "output35.png\n",
      "output36.png\n",
      "output37.png\n",
      "output38.png\n",
      "output39.png\n",
      "output4.png\n",
      "output40.png\n",
      "output41.png\n",
      "output42.png\n",
      "output43.png\n",
      "output44.png\n",
      "output45.png\n",
      "output46.png\n",
      "output47.png\n",
      "output48.png\n",
      "output49.png\n",
      "output5.png\n",
      "output50.png\n",
      "output51.png\n",
      "output52.png\n",
      "output53.png\n",
      "output54.png\n",
      "output55.png\n",
      "output56.png\n",
      "output57.png\n",
      "output58.png\n",
      "output59.png\n",
      "output6.png\n",
      "output60.png\n",
      "output61.png\n",
      "output62.png\n",
      "output63.png\n",
      "output64.png\n",
      "output65.png\n",
      "output66.png\n",
      "output67.png\n",
      "output68.png\n",
      "output69.png\n",
      "output7.png\n",
      "output70.png\n",
      "output71.png\n",
      "output72.png\n",
      "output73.png\n",
      "output74.png\n",
      "output75.png\n",
      "output76.png\n",
      "output77.png\n",
      "output78.png\n",
      "output79.png\n",
      "output8.png\n",
      "output80.png\n",
      "output81.png\n",
      "output82.png\n",
      "output83.png\n",
      "output84.png\n",
      "output85.png\n",
      "output86.png\n",
      "output87.png\n",
      "output88.png\n",
      "output89.png\n",
      "output9.png\n",
      "output90.png\n",
      "output91.png\n",
      "output92.png\n",
      "output93.png\n",
      "output94.png\n",
      "output95.png\n",
      "output96.png\n",
      "output97.png\n",
      "output98.png\n",
      "output99.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from PIL import Image\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"outputs/slice-outputs\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "dlopen(/opt/anaconda3/envs/keras-ADHD/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '_TF_GetInputPropertiesList'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, StratifiedKFold\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras-ADHD/lib/python3.8/site-packages/tensorflow/__init__.py:443\u001b[0m\n\u001b[1;32m    441\u001b[0m _plugin_dir \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow-plugins\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_plugin_dir):\n\u001b[0;32m--> 443\u001b[0m   \u001b[43m_ll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_plugin_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m   \u001b[38;5;66;03m# Load Pluggable Device Library\u001b[39;00m\n\u001b[1;32m    445\u001b[0m   _ll\u001b[38;5;241m.\u001b[39mload_pluggable_device_library(_plugin_dir)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/keras-ADHD/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py:151\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    148\u001b[0m     kernel_libraries \u001b[38;5;241m=\u001b[39m [library_location]\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m kernel_libraries:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m       errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    156\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe file or folder to load kernel libraries from does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m       library_location)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: dlopen(/opt/anaconda3/envs/keras-ADHD/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '_TF_GetInputPropertiesList'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Convolution2D, Conv2D, MaxPooling2D, Lambda, GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Activation, AveragePooling2D, Concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "%matplotlib inline\n",
    "keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU details:  {'device_name': 'METAL'}\n"
     ]
    }
   ],
   "source": [
    "devices = tf.config.list_physical_devices()\n",
    "# print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  details = tf.config.experimental.get_device_details(gpus[0])\n",
    "  print(\"GPU details: \", details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>658286</th>\n",
       "      <th>658287</th>\n",
       "      <th>658288</th>\n",
       "      <th>658289</th>\n",
       "      <th>658290</th>\n",
       "      <th>658291</th>\n",
       "      <th>658292</th>\n",
       "      <th>658293</th>\n",
       "      <th>658294</th>\n",
       "      <th>658295</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>255</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>255</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 658296 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1       2       3       4       5       6       7       8       \\\n",
       "0       3       3       3     255       3       3       3     255       3   \n",
       "1       2       2       2     255       2       2       2     255       2   \n",
       "2       2       2       2     255       2       2       2     255       2   \n",
       "3       2       2       2     255       2       2       2     255       2   \n",
       "4       3       3       3     255       3       3       3     255       3   \n",
       "\n",
       "   9       ...  658286  658287  658288  658289  658290  658291  658292  \\\n",
       "0       3  ...      10     255      10      10      10     255      10   \n",
       "1       2  ...       2     255       2       2       2     255       2   \n",
       "2       2  ...       1     255       1       1       1     255       1   \n",
       "3       2  ...       3     255       3       3       3     255       3   \n",
       "4       3  ...       2     255       2       2       2     255       2   \n",
       "\n",
       "   658293  658294  658295  \n",
       "0      10      10     255  \n",
       "1       2       2     255  \n",
       "2       1       1     255  \n",
       "3       3       3     255  \n",
       "4       2       2     255  \n",
       "\n",
       "[5 rows x 658296 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list = []\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = 'outputs/slice-outputs'\n",
    "\n",
    "# Get a list of all the image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "# Iterate through the image files\n",
    "for image_file in image_files:\n",
    "\n",
    "    # Open the image file\n",
    "    img = Image.open(os.path.join(folder_path, image_file))\n",
    "\n",
    "    # Convert the image to an array\n",
    "    img_array = np.array(img)\n",
    "    # Flatten the image array\n",
    "    img_array_flat = img_array.flatten()\n",
    "    # Append the flattened image array to the list\n",
    "    image_list.append(img_array_flat)\n",
    "\n",
    "# Create a dataframe from the list of flattened image arrays\n",
    "df = pd.DataFrame(image_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_kfold(k):\n",
    "    # Assuming the DataFrame df already contains flattened image arrays\n",
    "    X_train = df.values  # Get the image data from the DataFrame as numpy array\n",
    "    \n",
    "    # If your images are grayscale or RGB, you may need to reshape them back\n",
    "    # For example, if your images are 75x75:\n",
    "    # X_train = np.array([img.reshape(75, 75, 3) for img in X_train])  # for RGB images\n",
    "    # Or if grayscale:\n",
    "    # X_train = np.array([img.reshape(75, 75) for img in X_train])\n",
    "    \n",
    "    # y_train is your target variable (this needs to be defined if not in the DataFrame)\n",
    "    # For example, if the labels (is_iceberg) are in another list or DataFrame column:\n",
    "    # y_train = your_labels  # This should match the size of your data\n",
    "    \n",
    "    # Placeholder for example labels (binary classification 0 or 1)\n",
    "    y_train = np.random.randint(2, size=len(X_train))  # Replace with actual labels\n",
    "    \n",
    "    # Create stratified K-Folds\n",
    "    folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=1).split(X_train, y_train))\n",
    "    \n",
    "    return folds, X_train, y_train\n",
    "\n",
    "k = 7\n",
    "folds, X_train, y_train = load_data_kfold(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    x = Input((75, 75, 3))\n",
    "    model = BatchNormalization(axis = 3)(x)\n",
    "    model = Convolution2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same', activation='relu')(model)\n",
    "    model = MaxPooling2D()(model)\n",
    "    \n",
    "    model = BatchNormalization(axis = 3)(model)\n",
    "    model = Convolution2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', activation='relu')(model)\n",
    "    model = MaxPooling2D()(model)\n",
    "    \n",
    "    model = BatchNormalization(axis = 3)(model)\n",
    "    model = Convolution2D(filters = 128, kernel_size = (3,3), strides = (1,1), padding = 'same', activation='relu')(model)\n",
    "    model = MaxPooling2D()(model)\n",
    "    \n",
    "    model = BatchNormalization(axis = 3)(model)\n",
    "    model = Convolution2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', activation='relu')(model) \n",
    "    model = GlobalAveragePooling2D()(model)\n",
    " \n",
    "    model = Dense(1, activation = 'sigmoid')(model)\n",
    "    \n",
    "    model = Model(inputs = x, outputs = model)\n",
    "    \n",
    "    opt_adam = keras.optimizers.legacy.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(opt_adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 75, 75, 3)]       0         \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 75, 75, 3)         12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 75, 75, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 37, 37, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 37, 37, 32)        128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 37, 37, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 18, 18, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 18, 18, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 18, 18, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 9, 9, 128)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 9, 9, 128)         512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 9, 9, 64)          73792     \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 64)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 168013 (656.30 KB)\n",
      "Trainable params: 167559 (654.53 KB)\n",
      "Non-trainable params: 454 (1.77 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         zoom_range = 0.1,\n",
    "                         rotation_range = 10\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(name_weights, patience_lr):\n",
    "    mcp_save = ModelCheckpoint(name_weights, save_best_only=True, monitor='val_loss', mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=patience_lr, verbose=1, epsilon=1e-4, mode='min')\n",
    "    return [mcp_save, reduce_lr_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 658296)\n",
      "\n",
      "Fold  0\n",
      "Size of X_train_cv before reshaping: 109277136\n",
      "Expected size after reshaping: 657072\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 109277136 into shape (468,468,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Update the dimensions based on the actual shape determined from the output\u001b[39;00m\n\u001b[1;32m     15\u001b[0m height, width, channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m468\u001b[39m, \u001b[38;5;241m468\u001b[39m, \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Adjust based on actual dimensions from the output\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m X_train_cv \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m y_train_cv \u001b[38;5;241m=\u001b[39m y_train[train_idx]\n\u001b[1;32m     19\u001b[0m X_valid_cv \u001b[38;5;241m=\u001b[39m X_train[val_idx]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, height, width, channels)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 109277136 into shape (468,468,1)"
     ]
    }
   ],
   "source": [
    "# First, print the shape of the data to check the size of the flattened images\n",
    "print(X_train.shape)\n",
    "\n",
    "# Assuming the flattened images have a different size than 75x75x3, adjust the reshape accordingly\n",
    "# For example, if the images are grayscale and the size is 75x75:\n",
    "# Replace 75x75x3 with 75x75x1 if grayscale, or calculate the correct dimensions if otherwise\n",
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    print('\\nFold ', j)\n",
    "\n",
    "    # Check the size of the fold\n",
    "    print(f\"Size of X_train_cv before reshaping: {X_train[train_idx].size}\")\n",
    "    print(f\"Expected size after reshaping: {468*468*3}\")\n",
    "    \n",
    "    # Update the dimensions based on the actual shape determined from the output\n",
    "    height, width, channels = 468, 468, 1  # Adjust based on actual dimensions from the output\n",
    "\n",
    "    X_train_cv = X_train[train_idx].reshape(-1, height, width, channels)\n",
    "    y_train_cv = y_train[train_idx]\n",
    "    X_valid_cv = X_train[val_idx].reshape(-1, height, width, channels)\n",
    "    y_valid_cv = y_train[val_idx]\n",
    "    \n",
    "    name_weights = \"final_model_fold\" + str(j) + \"_weights.h5\"\n",
    "    callbacks = get_callbacks(name_weights=name_weights, patience_lr=10)\n",
    "    \n",
    "    generator = gen.flow(X_train_cv, y_train_cv, batch_size=batch_size)\n",
    "    \n",
    "    model = get_model()\n",
    "    \n",
    "    model.fit(\n",
    "        generator,\n",
    "        steps_per_epoch=len(X_train_cv) // batch_size,\n",
    "        epochs=15,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid_cv, y_valid_cv),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    print(model.evaluate(X_valid_cv, y_valid_cv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-ADHD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
